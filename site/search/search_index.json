{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#introduction","title":"Introduction","text":"<p>A video/audio transcript service implementating the algorithm described on this thread:</p> <p>https://www.reddit.com/r/LocalLLaMA/comments/1g2vhy3/creating_very_highquality_transcripts_with/</p>"},{"location":"#the-algorithm","title":"The algorithm","text":"<p>Transcription is performed by a series of transformation steps. Details are available in the Algorithm document.</p>"},{"location":"#usage","title":"Usage","text":""},{"location":"#setup","title":"Setup","text":"<p>Setup you environment. Use the supplied script to use on the command line <pre><code>export HF_TOKEN=&lt;HF_TOKEN&gt;\nexport OLLAMA_HOST=&lt;OLLAMA_HOST&gt;\n</code></pre></p>"},{"location":"#ingestionpy-cli-tool","title":"ingestion.py CLI tool","text":"<pre><code>python ingestion.py /mnt/data3/AI/software/VideoRAG/Lexington/DPa2iRgzadM.wav\n</code></pre> <p>Results are stored in</p> <pre><code>/mnt/data3/AI/software/VideoRAG/Lexington/DPa2iRgzadM.d\n</code></pre> <p>Primarily cache.md</p>"},{"location":"#you-tube","title":"You Tube","text":"<p>YouTube viodeos can be do be downloaded using the yt-dlp package as follows</p> <pre><code>pip install yt-dlp\nyt-dlp -o \"%(id)s.%(ext)s\" -S \"res:720\" https://www.youtube.com/live/FpC_Lp_Kq_0  -P .\nffmpeg -i file.mkv -q:a 0 -map a audio_output.wav\n</code></pre>"},{"location":"#package","title":"Package","text":""},{"location":"#setup-treeseg-configuration","title":"Setup treeseg configuration","text":"<p>More information is available on the (topic-treeseg repo)[https://github.com/geraldthewes/topic-treeseg.git]</p> <pre><code>from treeseg import Embeddings, ollama_embeddings\n\n# Build config\n# Configuration\nembeddings_config = Embeddings(\n    embeddings_func=ollama_embeddings, # openai_embeddings\n    headers={}, # forOpenAI\n    model=\"nomic-embed-text\",  # or \"text-embedding-ada-002\" for openai         \n    endpoint=os.getenv(\"OLLAMA_HOST\", \"\")   # \"https://api.openai.com/v1/embeddings\"\n)\nconfig = {\n    \"MIN_SEGMENT_SIZE\": 10,\n    \"LAMBDA_BALANCE\": 0,\n    \"UTTERANCE_EXPANSION_WIDTH\": 3,\n    \"EMBEDDINGS\": embeddings_config,\n    \"TEXT_KEY\": \"transcript\"\n}\n</code></pre>"},{"location":"#create-videotranscriber-instance","title":"Create VideoTranscriber instance","text":"<pre><code>   transcriber = VideoTranscriber(config)\n</code></pre>"},{"location":"#transcribe","title":"Transcribe","text":"<pre><code>    result, nouns_list = transcriber.transcribe_video(video_path)\n    result, headlines, summary = transcriber.topics(video_path, result, max_topics) \n    transcriber.format_transcript(video_path, result, nouns_list, headlines, summary)\n</code></pre> <p>max_topics sets the maximum topics you want the topic segmenter to create. The longer the video, the more topic can be discussed.</p>"},{"location":"#reference-api","title":"Reference API","text":"<p>Read the reference API</p>"},{"location":"#appendix-obsolete-sections","title":"Appendix - Obsolete sections","text":"<p>The code below is old and will probably be removed.</p>"},{"location":"#sentence-merging","title":"Sentence merging","text":"<pre><code>python -m spacy download en_core_web_sm\npython merge_transcript_segments.py   /mnt/data3/AI/data/Needham/2024-10-24.d/cache.raw_transcript /mnt/data3/AI/data/Needham/2024-10-24.d/cache.sentence_merge\n</code></pre>"},{"location":"#topic-segmentation","title":"Topic segmentation","text":"<pre><code>python  topic_segment.py --transcript-file   /mnt/data3/AI/data/Needham/2024-10-24.d/cache.final --output-file=/mnt/data3/AI/data/Needham/2024-10-24.d/cache.topics --segments=512\n</code></pre>"},{"location":"CONVENTIONS/","title":"CONVENTIONS","text":"<p>When writing code, you MUST follow these principles:</p> <ul> <li>Code should be easy to read and understand.</li> <li>Keep the code as simple as possible. Avoid unnecessary complexity.</li> <li>Use meaningful names for variables, functions, etc. Names should reveal intent.</li> <li>Functions should be small and do one thing well. They should not exceed a few lines.- Functions should have unit tests</li> <li>Function names should describe the action being performed.</li> <li>Functions should only do one thing </li> <li>Only use comments when necessary, as they can become outdated. Instead, strive to make the code self-explanatory.</li> <li>When comments are used, they should add useful information that is not readily apparent from the code itself.</li> <li>Properly handle errors and exceptions to ensure the software's robustness.</li> <li>Consider security implications of the code. Implement security best practices to protect against vulnerabilities and attacks.</li> <li>Adhere to these 4 principles of Functional Programming:</li> <li>Pure Functions</li> <li>Immutability</li> <li>Function Composition</li> <li>Declarative Code</li> </ul>"},{"location":"Planning/","title":"Introduction","text":"<p>This project implements a video transcriping workflow in a python using a mix of LLM and specialized AI models</p>"},{"location":"Planning/#overall-workflow","title":"Overall workflow","text":"<p>The workflow was inspired from this post on reddit: https://www.reddit.com/r/LocalLLaMA/comments/1g2vhy3/creating_very_highquality_transcripts_with/</p> <p>It currently consists of</p> <ol> <li> <p>Initial Transcription</p> </li> <li> <p>Use a state of the art  open-source model transcription model such as whisper or equivalent, for the first pass.</p> </li> <li>The transcript consists of an array of transcript chunks in JSON that contains start and end time in seconds from start and the transcribed text</li> </ol> <pre><code>   {\n        \"start\": 49.84,\n        \"end\": 55.040000000000006,\n        \"transcript\": \" please recite the Pledge of Allegiance. I pledge a\"\n    },\n</code></pre> <ol> <li>Sentence merging </li> </ol> <p>Reassemble transcripted chunks into complete sentences</p> <ul> <li>Use a state of the art sentence merge model such as provide by Spacy</li> <li> <p>Attempt to keep timestamps</p> </li> <li> <p>Entity Extraction (NER)</p> </li> <li> <p>This step is important. Basically the problem is the raw transcript above will have mostly likely have the nouns and special (technical) terms wrong. You need to correct that. But before that you need to collect this special words? How...?</p> </li> <li> <p>Use structured API responses from open-source LLMs (like Outlines) or specialized models to extract a list of nouns from a master document. If you don't want to use open-source tools here, almost all commerical APIs offer structure API response too. You can use that too.</p> </li> <li> <p>In our case, for our podcast, we maintain a master document per episode that is basically like a script (for different uses) that contains all proper nouns, special technial terms and such? How do we extract that.</p> </li> <li> <p>We just simply dump that into a LLM (with a structured generation) and it give back an proper array list of special words that we need to keep an eye on, or we use a specialzied name entity recognition (NER) model.</p> </li> <li> <p>Prompt: \"Extract all proper nouns, technical terms, and important concepts from this text. Return as a JSON list.\" with Structure Generation. Something like that...</p> </li> <li> <p>Transcript Correction</p> </li> <li> <p>Feed the initial transcript and extracted noun list to your LLM.</p> </li> <li> <p>Prompt: \"Correct this transcript, paying special attention to the proper nouns and terms in the provided list. Ensure proper punctuation and formatting.\" (That is not the real prompt, but you get the idea...)</p> </li> <li> <p>Input: Raw transcript + noun list</p> </li> <li> <p>Output: Cleaned-up transcript</p> </li> <li> <p>Speaker Identification</p> </li> <li> <p>Use pyannote.audio (open source!) for speaker diarization.</p> </li> <li> <p>Bonus: Prompt your LLM to map speaker labels to actual names based on context.</p> </li> <li> <p>Topic segmentation</p> </li> <li> <p>As LLM have a harder time with longer text, break the transcript into topics using a state of the art topic segmntation model such as TreeSeg</p> </li> <li> <p>Final Formatting</p> </li> <li> <p>Use a simple script to format the transcript into your desired output (e.g., Markdown -&gt; With speaker labels and timing if you want). And just publish.</p> </li> </ul>"},{"location":"todo/","title":"Todo","text":"<p>[x] Also run soundex on the entities [X] Investigate speaker mapping [X] Fix bug in speaker section [X] Generate topic headers [ ] Create a MCP server in docker   [X] Move treeseg to it's own repo   [X] Make transcription it's own python package   [ ] Create new repo/project for server [X] In unit test - clear the cache directory [ ] Cleanup documentation [ ] Move ollama models in the configuration [ ] Handle non wav files in the diarization step (aka vidoes) [ ] Add vision models to extract material from the video</p>"},{"location":"api/mst/","title":"mst Package","text":""},{"location":"api/mst/#mst","title":"mst","text":"<p>Multi-Step Transcriber (mst) Package</p> <p>This package provides a comprehensive suite of tools for generating high-quality video and audio transcripts. It implements an automated workflow that leverages Large Language Models (LLMs) and specialized audio processing libraries to achieve accurate and well-formatted results.</p> <p>The core process involves several key steps: 1. Initial Transcription: Generating a raw transcript from the media file. 2. Noun and Entity Extraction: Identifying important terms and proper nouns    to improve accuracy. 3. Transcript Correction: Refining the raw transcript using the extracted    entities and LLMs. 4. Speaker Diarization: Identifying and labeling different speakers in    the audio. 5. Topic Segmentation: Dividing the transcript into logical topic segments    and generating headlines/summaries. 6. Formatting: Producing final outputs in various formats, such as    Markdown, including speaker labels and timestamps.</p> <p>This package aims to provide a flexible, cost-effective, and controllable solution for transcription tasks, allowing for customization and continuous improvement.</p>"},{"location":"api/video_transcriber/","title":"VideoTranscriber","text":""},{"location":"api/video_transcriber/#mst.VideoTranscriber","title":"mst.VideoTranscriber","text":"<pre><code>VideoTranscriber(topic_config=None)\n</code></pre> <p>A class to handle the end-to-end video transcription process, including initial transcription, noun extraction, correction, speaker diarization, topic segmentation, and formatting.</p> <p>This class orchestrates the complete transcription pipeline, managing the flow of data between different processing steps.</p> <p>Initializes the VideoTranscriber.</p> <p>Parameters:</p> Name Type Description Default <code>topic_config</code> <code>dict</code> <p>Configuration for topic segmentation.                            Defaults to None, in which case topic                            segmentation will be skipped.</p> <code>None</code> Source code in <code>mst/video_transcriber.py</code> <pre><code>def __init__(self, topic_config=None):\n    \"\"\"\n    Initializes the VideoTranscriber.\n\n    Args:\n        topic_config (dict, optional): Configuration for topic segmentation.\n                                       Defaults to None, in which case topic\n                                       segmentation will be skipped.\n    \"\"\"\n    # Initialize models\n    self.topic_config = topic_config\n</code></pre>"},{"location":"api/video_transcriber/#mst.VideoTranscriber.transcribe_video","title":"transcribe_video","text":"<pre><code>transcribe_video(video_path, transcribe=True)\n</code></pre> <p>Processes a video file through the complete transcription pipeline.</p> <p>The pipeline includes: 1. Initial transcription using a speech-to-text model. 2. Merging transcript segments into sentences. 3. Extracting nouns and important terms. 4. Correcting the transcript using the extracted nouns. 5. Identifying speakers (diarization). 6. Merging diarization information with the transcript. 7. Compressing the transcript by combining consecutive segments from the same speaker. 8. Finding speaker introductions in the raw transcript. 9. Creating a map of speaker IDs to actual names based on introductions. 10. Applying the speaker names to the final transcript.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>The file path to the video or audio file.</p> required <code>transcribe</code> <code>bool</code> <p>False to skip audio transcription, default True</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing: - transcript_final (list): The final processed transcript with speaker information. - nouns_list (list): A list of extracted nouns and entities.</p> Source code in <code>mst/video_transcriber.py</code> <pre><code>def transcribe_video(self, video_path: str, transcribe: bool = True) -&gt; tuple:\n    \"\"\"\n    Processes a video file through the complete transcription pipeline.\n\n    The pipeline includes:\n    1. Initial transcription using a speech-to-text model.\n    2. Merging transcript segments into sentences.\n    3. Extracting nouns and important terms.\n    4. Correcting the transcript using the extracted nouns.\n    5. Identifying speakers (diarization).\n    6. Merging diarization information with the transcript.\n    7. Compressing the transcript by combining consecutive segments from the same speaker.\n    8. Finding speaker introductions in the raw transcript.\n    9. Creating a map of speaker IDs to actual names based on introductions.\n    10. Applying the speaker names to the final transcript.\n\n    Args:\n        video_path (str): The file path to the video or audio file.\n        transcribe(bool): False to skip audio transcription, default True\n\n    Returns:\n        tuple: A tuple containing:\n            - transcript_final (list): The final processed transcript with speaker information.\n            - nouns_list (list): A list of extracted nouns and entities.\n    \"\"\"\n    if transcribe:\n        print('Step 1: Initial transcription')\n        raw_transcript = initial_transcription(video_path)\n    else:\n        # assume transcription already completed\n        print('Skipping transcription step')\n        # Load the cached raw transcript\n        cache_file = get_cache_file(video_path, '.raw_transcript')\n        raw_transcript = load_object_file(cache_file)\n\n    print('Step 2: Merge Sentences')\n    merged_segments = merge_transcript_segments(video_path, raw_transcript)\n\n    print('Step 3: Entity extraction')\n    nouns_list = extract_nouns(video_path, merged_segments)\n\n    print('Step 4: Transcript correction')\n    corrected_transcript = correct_transcript(video_path, merged_segments, nouns_list)\n    # print(corrected_transcript)\n\n    print('Step 5: Diarization / Speaker identification')\n    speaker_mapping = identify_speakers(video_path, corrected_transcript)\n    #print(speaker_mapping)\n\n    print('Step 6: Merge transcript and diarization')\n    merged_transcript = merge_transcript_diarization(video_path, corrected_transcript,  speaker_mapping )\n\n    print('Step 7: Compress merged transcript')\n    compressed_transcript = compress_transcript(video_path, merged_transcript)\n\n    print('Step 8: Filter transcript by speaker introductions')\n    # Use raw transcript as sentence merge can cause timing mismatch\n    speaker_introductions = find_introductions(video_path, raw_transcript)\n\n    print('Step 9: Extract persons from introductions')\n    speaker_map = create_speaker_map(video_path, speaker_introductions, speaker_mapping)\n\n    print('Step 10: Map speaker names')\n    transcript_final = map_speakers(video_path, compressed_transcript, speaker_map)\n\n    return transcript_final, nouns_list\n</code></pre>"},{"location":"api/video_transcriber/#mst.VideoTranscriber.topics","title":"topics","text":"<pre><code>topics(video_path, transcript, max_topics)\n</code></pre> <p>Segments the transcript into topics and generates headlines and summaries for them.</p> <p>If <code>topic_config</code> was not provided during initialization, this step is skipped.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>The file path to the video or audio file, used for caching.</p> required <code>transcript</code> <code>list</code> <p>The transcript to be segmented (typically the output of <code>transcribe_video</code>).</p> required <code>max_topics</code> <code>int</code> <p>The maximum number of topics to segment the transcript into.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing: - processed_transcript (list): The transcript with topic information. - topic_headlines (list): A list of generated headlines for each topic. - topic_summary (list): A list of generated summaries for each topic.</p> <code>tuple</code> <p>If topic segmentation is skipped, returns the original transcript and two empty lists.</p> Source code in <code>mst/video_transcriber.py</code> <pre><code>def topics(self, video_path: str, transcript: list, max_topics: int) -&gt; tuple:\n    \"\"\"\n    Segments the transcript into topics and generates headlines and summaries for them.\n\n    If `topic_config` was not provided during initialization, this step is skipped.\n\n    Args:\n        video_path (str): The file path to the video or audio file, used for caching.\n        transcript (list): The transcript to be segmented (typically the output of `transcribe_video`).\n        max_topics (int): The maximum number of topics to segment the transcript into.\n\n    Returns:\n        tuple: A tuple containing:\n            - processed_transcript (list): The transcript with topic information.\n            - topic_headlines (list): A list of generated headlines for each topic.\n            - topic_summary (list): A list of generated summaries for each topic.\n        If topic segmentation is skipped, returns the original transcript and two empty lists.\n    \"\"\"\n    if not self.topic_config:\n        print('No topic segmentation configuration. Skipping topic segmentation')\n        return transcript, [], []\n    processed_transcript = segment_topics(video_path, transcript, self.topic_config, max_topics)\n    # Generate and cache topic headlines\n    topic_headlines = prepare_and_generate_headlines(video_path, processed_transcript)\n    topic_summary = prepare_and_generate_summary(video_path, processed_transcript)\n\n    return processed_transcript, topic_headlines, topic_summary\n</code></pre>"},{"location":"api/video_transcriber/#mst.VideoTranscriber.format_transcript","title":"format_transcript","text":"<pre><code>format_transcript(\n    video_path,\n    transcript,\n    nouns_list,\n    topic_headlines,\n    topic_summary,\n)\n</code></pre> <p>Formats the processed transcript into plain text and Markdown.</p> <p>The formatted outputs are cached to files.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>The file path to the video or audio file, used for caching.</p> required <code>transcript</code> <code>list</code> <p>The final transcript data.</p> required <code>nouns_list</code> <code>Dict[str, List[Dict[str, Any]]]</code> <p>A dictionary of extracted nouns/entities.</p> required <code>topic_headlines</code> <code>list</code> <p>A list of topic headlines.</p> required <code>topic_summary</code> <code>list</code> <p>A list of topic summaries.</p> required Source code in <code>mst/video_transcriber.py</code> <pre><code>def format_transcript(self,\n                      video_path: str,\n                      transcript: list,\n                      nouns_list:  Dict[str, List[Dict[str, Any]]],\n                      topic_headlines: list,\n                      topic_summary: list) -&gt; None:\n    \"\"\"\n    Formats the processed transcript into plain text and Markdown.\n\n    The formatted outputs are cached to files.\n\n    Args:\n        video_path (str): The file path to the video or audio file, used for caching.\n        transcript (list): The final transcript data.\n        nouns_list (Dict[str, List[Dict[str, Any]]]): A dictionary of extracted nouns/entities.\n        topic_headlines (list): A list of topic headlines.\n        topic_summary (list): A list of topic summaries.\n    \"\"\"\n    transcript_formatted = format_transcript(video_path, transcript)\n    transcript_markdown = format_markdown(video_path, transcript, nouns_list, topic_headlines, topic_summary)\n</code></pre>"},{"location":"api/video_transcriber/#mst.VideoTranscriber.retrieve_json","title":"retrieve_json","text":"<pre><code>retrieve_json(video_path)\n</code></pre> <p>Retrieves the cached transcript with topic segmentation as a JSON string.</p> <p>This typically loads the '.topics' cached file.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>The file path to the video or audio file, used to locate the cache.</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: The JSON string content of the cached topics file, or None if not found.</p> Source code in <code>mst/video_transcriber.py</code> <pre><code>def retrieve_json(self, video_path: str) -&gt; str | None:\n    \"\"\"\n    Retrieves the cached transcript with topic segmentation as a JSON string.\n\n    This typically loads the '.topics' cached file.\n\n    Args:\n        video_path (str): The file path to the video or audio file, used to locate the cache.\n\n    Returns:\n        str | None: The JSON string content of the cached topics file, or None if not found.\n    \"\"\"\n    path = get_cache_file(video_path, EXTENSION_TOPICS)\n    return load_text_file(path)\n</code></pre>"},{"location":"api/video_transcriber/#mst.VideoTranscriber.retrieve_markdown","title":"retrieve_markdown","text":"<pre><code>retrieve_markdown(video_path)\n</code></pre> <p>Retrieves the cached transcript formatted as Markdown.</p> <p>This typically loads the '.md' cached file.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>The file path to the video or audio file, used to locate the cache.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>Any | None: The content of the cached Markdown file (often a string),         or None if not found. The return type depends on <code>load_object_file</code>.</p> Source code in <code>mst/video_transcriber.py</code> <pre><code>def retrieve_markdown(self, video_path: str) -&gt; Any | None:\n    \"\"\"\n    Retrieves the cached transcript formatted as Markdown.\n\n    This typically loads the '.md' cached file.\n\n    Args:\n        video_path (str): The file path to the video or audio file, used to locate the cache.\n\n    Returns:\n        Any | None: The content of the cached Markdown file (often a string),\n                    or None if not found. The return type depends on `load_object_file`.\n    \"\"\"\n    path = get_cache_file(video_path, EXTENSION_MARKDOWN)\n    return load_text_file(path)\n</code></pre>"},{"location":"api/video_transcriber/#mst.VideoTranscriber.clear","title":"clear","text":"<pre><code>clear(video_path)\n</code></pre> <p>Clears all cached files associated with the given video path.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>The file path to the video or audio file whose cache should be cleared.</p> required Source code in <code>mst/video_transcriber.py</code> <pre><code>def clear(self, video_path: str):\n    \"\"\"\n    Clears all cached files associated with the given video path.\n\n    Args:\n        video_path (str): The file path to the video or audio file whose cache should be cleared.\n    \"\"\"\n    return clear_cache_directory(video_path)\n</code></pre>"}]}